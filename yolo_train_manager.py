"""
YOLOè®­ç»ƒç®¡ç†å™¨ - ç»Ÿä¸€çš„è®­ç»ƒã€å‡†å¤‡ã€æµ‹è¯•è„šæœ¬

âš ï¸  é‡è¦è¯´æ˜ï¼š
    è¿™æ˜¯ä¸€é”®è®­ç»ƒè„šæœ¬ï¼Œç”¨äºæ‰€æœ‰é¡µé¢å…ƒç´ çš„YOLOæ¨¡å‹è®­ç»ƒ
    æ–‡ä»¶åï¼šyolo_train_manager.py
    âŒ ç¦æ­¢åˆ é™¤æ­¤æ–‡ä»¶ï¼åæœŸè®­ç»ƒæ–°æ¨¡å‹æ—¶éœ€è¦ä½¿ç”¨
    
åŠŸèƒ½ï¼š
  1. prepare  - å‡†å¤‡æ•°æ®é›†ï¼ˆè‡ªåŠ¨æ•°æ®å¢å¼ºã€åˆ’åˆ†è®­ç»ƒé›†/éªŒè¯é›†ï¼‰
  2. train    - è®­ç»ƒæ¨¡å‹ï¼ˆGPUåŠ é€Ÿã€è‡ªåŠ¨æ—©åœï¼‰
  3. test     - æµ‹è¯•æ¨¡å‹ï¼ˆå…¨é‡æµ‹è¯•+éšæœºæŠ½æ ·10å¼ æˆªå›¾ï¼‰
  4. cleanup  - è®­ç»ƒåæ•´ç†ï¼ˆä¿å­˜åŸå§‹å›¾ã€åˆ é™¤å¢å¼ºæ•°æ®ã€æ³¨å†Œæ¨¡å‹ï¼‰
  5. all      - å®Œæ•´æµç¨‹ï¼ˆå‡†å¤‡â†’è®­ç»ƒâ†’æµ‹è¯•â†’æ•´ç†ï¼‰

ç”¨æ³•ï¼š
  python yolo_train_manager.py prepare åˆ†ç±»é¡µ    # å‡†å¤‡æ•°æ®é›†
  python yolo_train_manager.py train åˆ†ç±»é¡µ --epochs 30      # è®­ç»ƒæ¨¡å‹ï¼ˆæŒ‡å®šè½®æ•°ï¼‰
  python yolo_train_manager.py test åˆ†ç±»é¡µ       # æµ‹è¯•æ¨¡å‹
  python yolo_train_manager.py cleanup åˆ†ç±»é¡µ    # è®­ç»ƒåæ•´ç†
  python yolo_train_manager.py all åˆ†ç±»é¡µ        # å®Œæ•´æµç¨‹

å·²å®Œæˆè®­ç»ƒçš„æ¨¡å‹ï¼š
  - åˆ†ç±»é¡µã€æœç´¢é¡µã€ç§¯åˆ†é¡µã€æ–‡ç« é¡µã€é’±åŒ…é¡µ
  - ä¸ªäººé¡µå¹¿å‘Šã€é¦–é¡µå¼‚å¸¸ä»£ç å¼¹çª—
  - å…¶ä»–20+ä¸ªé¡µé¢å…ƒç´ æ£€æµ‹æ¨¡å‹
"""
import json
import shutil
import random
import glob
import cv2
import os
import subprocess
import platform
from pathlib import Path
from PIL import Image, ImageEnhance
import yaml
import torch
from ultralytics import YOLO
from datetime import datetime
import argparse


class YOLOTrainManager:
    """YOLOè®­ç»ƒç®¡ç†å™¨"""
    
    def __init__(self, page_type):
        self.page_type = page_type
        self.source_dir = Path(f"training_data/{page_type}")
        self.dataset_dir = Path(f"yolo_dataset_{page_type}")
        self.model_name = f"{page_type}_detector"
        
    def check_annotations(self):
        """æ£€æŸ¥æ ‡æ³¨æ•°æ®"""
        annotation_file = self.source_dir / "annotations.json"
        if not annotation_file.exists():
            print(f"âŒ æ‰¾ä¸åˆ°æ ‡æ³¨æ–‡ä»¶: {annotation_file}")
            return None
        
        with open(annotation_file, 'r', encoding='utf-8') as f:
            annotations = json.load(f)
        
        total = len(annotations)
        annotated = sum(1 for v in annotations.values() if v)
        unannotated = total - annotated
        
        # è·å–æ‰€æœ‰ç±»åˆ«
        classes = set()
        for boxes in annotations.values():
            if boxes:
                for box in boxes:
                    classes.add(box['class'])
        
        print(f"\nğŸ“Š æ ‡æ³¨æ•°æ®ç»Ÿè®¡:")
        print(f"  æ€»å›¾ç‰‡æ•°: {total}")
        print(f"  å·²æ ‡æ³¨: {annotated}")
        print(f"  æœªæ ‡æ³¨: {unannotated}")
        print(f"  ç±»åˆ«: {sorted(classes)}")
        
        return {
            'total': total,
            'annotated': annotated,
            'unannotated': unannotated,
            'classes': sorted(classes),
            'annotations': annotations
        }
    
    def augment_image(self, image_path, output_dir, base_name, annotations, augment_factor=15):
        """æ•°æ®å¢å¼º"""
        img = Image.open(image_path)
        augmented_data = []
        
        # 1. åŸå›¾
        original_path = output_dir / f"{base_name}_original.png"
        img.save(original_path)
        augmented_data.append((str(original_path), annotations))
        
        # æ ¹æ®å¢å¼ºå€æ•°ç”Ÿæˆä¸åŒçš„å¢å¼º
        if augment_factor >= 5:
            # äº®åº¦è°ƒæ•´
            for i, factor in enumerate([0.7, 0.85, 1.15, 1.3], 1):
                enhancer = ImageEnhance.Brightness(img)
                bright_img = enhancer.enhance(factor)
                path = output_dir / f"{base_name}_bright_{i}.png"
                bright_img.save(path)
                augmented_data.append((str(path), annotations))
        
        if augment_factor >= 10:
            # å¯¹æ¯”åº¦è°ƒæ•´
            for i, factor in enumerate([0.6, 0.8, 1.2, 1.4], 1):
                enhancer = ImageEnhance.Contrast(img)
                contrast_img = enhancer.enhance(factor)
                path = output_dir / f"{base_name}_contrast_{i}.png"
                contrast_img.save(path)
                augmented_data.append((str(path), annotations))
        
        if augment_factor >= 15:
            # è‰²å½©å’Œé”åº¦
            for i, factor in enumerate([0.7, 1.15, 1.3], 1):
                enhancer = ImageEnhance.Color(img)
                color_img = enhancer.enhance(factor)
                path = output_dir / f"{base_name}_color_{i}.png"
                color_img.save(path)
                augmented_data.append((str(path), annotations))
            
            for i, factor in enumerate([0.5, 1.3], 1):
                enhancer = ImageEnhance.Sharpness(img)
                sharp_img = enhancer.enhance(factor)
                path = output_dir / f"{base_name}_sharp_{i}.png"
                sharp_img.save(path)
                augmented_data.append((str(path), annotations))
        
        # åªè¿”å›éœ€è¦çš„æ•°é‡
        return augmented_data[:augment_factor]
    
    def prepare_dataset(self, augment_factor=None):
        """å‡†å¤‡YOLOæ•°æ®é›†"""
        print(f"\n{'='*60}")
        print(f"å‡†å¤‡ {self.page_type} æ•°æ®é›†")
        print(f"{'='*60}\n")
        
        # æ£€æŸ¥æ ‡æ³¨
        data_info = self.check_annotations()
        if not data_info:
            return False
        
        annotations = data_info['annotations']
        annotated_count = data_info['annotated']
        
        # è‡ªåŠ¨é€‰æ‹©å¢å¼ºå€æ•°
        if augment_factor is None:
            if annotated_count < 20:
                augment_factor = 20
            elif annotated_count < 50:
                augment_factor = 15
            elif annotated_count < 100:
                augment_factor = 10
            else:
                augment_factor = 5
        
        print(f"\nğŸ“¦ æ•°æ®å¢å¼ºé…ç½®:")
        print(f"  åŸå§‹å›¾ç‰‡: {annotated_count}å¼ ")
        print(f"  å¢å¼ºå€æ•°: {augment_factor}x")
        print(f"  é¢„è®¡ç”Ÿæˆ: {annotated_count * augment_factor}å¼ ")
        
        # åˆ›å»ºä¸´æ—¶å¢å¼ºç›®å½•
        temp_dir = Path(f"training_data/{self.page_type}_temp_augmented")
        temp_dir.mkdir(exist_ok=True)
        
        # æ•°æ®å¢å¼º
        print(f"\nğŸ¨ å¼€å§‹æ•°æ®å¢å¼º...")
        augmented_annotations = {}
        for image_path_str, anns in annotations.items():
            if not anns:
                continue
            
            image_path = Path(image_path_str)
            if not image_path.exists():
                continue
            
            base_name = image_path.stem
            augmented_list = self.augment_image(image_path, temp_dir, base_name, anns, augment_factor)
            
            for aug_path, aug_anns in augmented_list:
                augmented_annotations[aug_path] = aug_anns
        
        print(f"  âœ“ ç”Ÿæˆäº† {len(augmented_annotations)} å¼ å¢å¼ºå›¾ç‰‡")
        
        # åˆ›å»ºYOLOæ•°æ®é›†ç›®å½•
        self.dataset_dir.mkdir(exist_ok=True)
        for split in ['train', 'val']:
            (self.dataset_dir / "images" / split).mkdir(parents=True, exist_ok=True)
            (self.dataset_dir / "labels" / split).mkdir(parents=True, exist_ok=True)
        
        # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
        all_images = list(augmented_annotations.keys())
        random.seed(42)
        random.shuffle(all_images)
        
        split_idx = int(len(all_images) * 0.8)
        train_images = all_images[:split_idx]
        val_images = all_images[split_idx:]
        
        print(f"\nğŸ“‚ åˆ’åˆ†æ•°æ®é›†:")
        print(f"  è®­ç»ƒé›†: {len(train_images)}å¼ ")
        print(f"  éªŒè¯é›†: {len(val_images)}å¼ ")
        
        # åˆ›å»ºç±»åˆ«æ˜ å°„
        classes = data_info['classes']
        class_to_id = {cls: idx for idx, cls in enumerate(classes)}
        
        # è½¬æ¢ä¸ºYOLOæ ¼å¼
        print(f"\nğŸ”„ è½¬æ¢ä¸ºYOLOæ ¼å¼...")
        for split, images in [('train', train_images), ('val', val_images)]:
            for img_path in images:
                img = Image.open(img_path)
                img_width, img_height = img.size
                
                # å¤åˆ¶å›¾ç‰‡
                img_name = Path(img_path).name
                shutil.copy2(img_path, self.dataset_dir / "images" / split / img_name)
                
                # åˆ›å»ºæ ‡ç­¾æ–‡ä»¶
                label_path = self.dataset_dir / "labels" / split / Path(img_name).with_suffix(".txt")
                with open(label_path, 'w') as f:
                    for box in augmented_annotations[img_path]:
                        class_id = class_to_id[box['class']]
                        
                        # å¤„ç†ä¸¤ç§æ ‡æ³¨æ ¼å¼
                        if 'x' in box and 'width' in box:
                            # æ ¼å¼1: x, y, width, height
                            x_center = (box['x'] + box['width'] / 2) / img_width
                            y_center = (box['y'] + box['height'] / 2) / img_height
                            width = box['width'] / img_width
                            height = box['height'] / img_height
                        elif 'x1' in box and 'x2' in box:
                            # æ ¼å¼2: x1, y1, x2, y2
                            x_center = ((box['x1'] + box['x2']) / 2) / img_width
                            y_center = ((box['y1'] + box['y2']) / 2) / img_height
                            width = (box['x2'] - box['x1']) / img_width
                            height = (box['y2'] - box['y1']) / img_height
                        else:
                            print(f"  âš  æœªçŸ¥çš„æ ‡æ³¨æ ¼å¼: {box}")
                            continue
                        
                        f.write(f"{class_id} {x_center} {y_center} {width} {height}\n")
        
        # åˆ›å»ºdataset.yaml
        dataset_yaml = {
            'path': str(self.dataset_dir.absolute()),
            'train': 'images/train',
            'val': 'images/val',
            'names': {idx: cls for idx, cls in enumerate(classes)},
            'nc': len(classes)
        }
        
        yaml_path = self.dataset_dir / "dataset.yaml"
        with open(yaml_path, 'w', encoding='utf-8') as f:
            yaml.dump(dataset_yaml, f, allow_unicode=True)
        
        # æ¸…ç†ä¸´æ—¶ç›®å½•
        try:
            shutil.rmtree(temp_dir)
        except PermissionError:
            print(f"  âš  æ— æ³•åˆ é™¤ä¸´æ—¶ç›®å½•ï¼ˆæ–‡ä»¶è¢«å ç”¨ï¼‰ï¼Œè¯·æ‰‹åŠ¨åˆ é™¤: {temp_dir}")
        except Exception as e:
            print(f"  âš  æ¸…ç†ä¸´æ—¶ç›®å½•æ—¶å‡ºé”™: {e}")
        
        print(f"\nâœ… æ•°æ®é›†å‡†å¤‡å®Œæˆ!")
        print(f"  ä½ç½®: {self.dataset_dir}")
        print(f"  é…ç½®: {yaml_path}")
        
        return True
    
    def train_model(self, epochs=50):
        """è®­ç»ƒæ¨¡å‹"""
        print(f"\n{'='*60}")
        print(f"è®­ç»ƒ {self.page_type} æ¨¡å‹")
        print(f"{'='*60}\n")
        
        # æ£€æŸ¥æ•°æ®é›†
        yaml_path = self.dataset_dir / "dataset.yaml"
        if not yaml_path.exists():
            print(f"âŒ æ•°æ®é›†ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œ prepare")
            return False
        
        # æ£€æŸ¥GPU
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        print(f"ğŸ–¥ï¸  è®¾å¤‡ä¿¡æ¯:")
        print(f"  ä½¿ç”¨è®¾å¤‡: {device}")
        if device == 'cuda':
            print(f"  GPUåç§°: {torch.cuda.get_device_name(0)}")
            print(f"  GPUæ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
        
        # è®­ç»ƒé…ç½®
        print(f"\nâš™ï¸  è®­ç»ƒé…ç½®:")
        print(f"  æ¨¡å‹: YOLOv8n")
        print(f"  è½®æ•°: {epochs}")
        print(f"  æ‰¹æ¬¡: 16")
        print(f"  å›¾ç‰‡å¤§å°: 640")
        print(f"  æ—©åœ: patience=50")
        print(f"  Workers: 0 (Windows)")
        
        # åŠ è½½æ¨¡å‹
        model = YOLO('yolov8n.pt')
        
        # å¼€å§‹è®­ç»ƒ
        print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ...")
        results = model.train(
            data=str(yaml_path),
            epochs=epochs,
            imgsz=640,
            batch=16,
            device=0 if device == 'cuda' else 'cpu',
            workers=0,  # Windowsç³»ç»Ÿä½¿ç”¨0
            cache=True,
            amp=True,
            patience=50,
            save=True,
            project='runs/detect/yolo_runs',
            name=self.model_name,
            verbose=True
        )
        
        print(f"\nâœ… è®­ç»ƒå®Œæˆ!")
        return True
    
    def test_model(self):
        """æµ‹è¯•æ¨¡å‹"""
        print(f"\n{'='*60}")
        print(f"æµ‹è¯• {self.page_type} æ¨¡å‹")
        print(f"{'='*60}\n")
        
        # æŸ¥æ‰¾æœ€æ–°çš„æ¨¡å‹
        model_pattern = f"runs/detect/**/yolo_runs/{self.model_name}*/weights/best.pt"
        model_files = glob.glob(model_pattern, recursive=True)
        
        if not model_files:
            print(f"âŒ æ‰¾ä¸åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹")
            return None
        
        # ä½¿ç”¨æœ€æ–°çš„æ¨¡å‹
        model_path = sorted(model_files, key=lambda x: Path(x).stat().st_mtime)[-1]
        print(f"ğŸ“¦ åŠ è½½æ¨¡å‹: {model_path}")
        
        model = YOLO(model_path)
        
        # æµ‹è¯•æ•°æ®é›†
        yaml_path = self.dataset_dir / "dataset.yaml"
        if not yaml_path.exists():
            print(f"âŒ æ•°æ®é›†ä¸å­˜åœ¨")
            return None
        
        print(f"\nğŸ§ª å¼€å§‹æµ‹è¯•...")
        results = model.val(data=str(yaml_path))
        
        # ç»Ÿè®¡æ•°æ®é›†ä¿¡æ¯
        train_images = list((self.dataset_dir / "images" / "train").glob("*.png"))
        train_images += list((self.dataset_dir / "images" / "train").glob("*.jpg"))
        val_images = list((self.dataset_dir / "images" / "val").glob("*.png"))
        val_images += list((self.dataset_dir / "images" / "val").glob("*.jpg"))
        all_images = train_images + val_images
        
        print(f"\nğŸ“Š æµ‹è¯•ç»“æœ:")
        print(f"  mAP50: {results.box.map50:.3f}")
        print(f"  mAP50-95: {results.box.map:.3f}")
        print(f"  Precision: {results.box.mp:.3f}")
        print(f"  Recall: {results.box.mr:.3f}")
        print(f"\nğŸ“¦ æ•°æ®é›†ç»Ÿè®¡:")
        print(f"  è®­ç»ƒé›†: {len(train_images)}å¼ ")
        print(f"  éªŒè¯é›†: {len(val_images)}å¼ ")
        print(f"  æ€»è®¡: {len(all_images)}å¼ ï¼ˆåŒ…æ‹¬å¢å¼ºå›¾ï¼‰")
        
        # è¯»å–ç±»åˆ«ä¿¡æ¯
        with open(yaml_path, 'r', encoding='utf-8') as f:
            dataset_info = yaml.safe_load(f)
        classes = list(dataset_info['names'].values())
        num_classes = len(classes)
        
        # æµ‹è¯•æ‰€æœ‰å›¾ç‰‡å¹¶ç»Ÿè®¡æ£€æµ‹ç»“æœ
        print(f"\nğŸ” æµ‹è¯•æ‰€æœ‰å›¾ç‰‡çš„æ£€æµ‹æƒ…å†µ...")
        all_detection_stats = []
        fully_detected_count = 0
        
        for i, img_path in enumerate(all_images, 1):
            if i % 100 == 0:
                print(f"  è¿›åº¦: {i}/{len(all_images)}...")
            
            # é¢„æµ‹
            pred_results = model.predict(str(img_path), conf=0.25, save=False, verbose=False)
            
            # ç»Ÿè®¡æ£€æµ‹ç»“æœ
            detections = pred_results[0].boxes
            detected_classes = {}
            for cls_id in detections.cls:
                cls_name = classes[int(cls_id)]
                detected_classes[cls_name] = detected_classes.get(cls_name, 0) + 1
            
            # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ç±»åˆ«éƒ½è¢«æ£€æµ‹åˆ°
            missing_classes = [cls for cls in classes if cls not in detected_classes]
            
            all_detection_stats.append({
                'image': img_path.name,
                'detected': detected_classes,
                'missing': missing_classes,
                'total_detections': len(detections)
            })
            
            if not missing_classes:
                fully_detected_count += 1
        
        # ç»Ÿè®¡æ€»ä½“æ£€æµ‹æƒ…å†µ
        total_images = len(all_detection_stats)
        partially_detected = total_images - fully_detected_count
        
        print(f"\n  ğŸ“Š å…¨éƒ¨å›¾ç‰‡æ£€æµ‹ç»Ÿè®¡:")
        print(f"    æµ‹è¯•å›¾ç‰‡: {total_images}å¼ ")
        print(f"    å…¨éƒ¨æ£€æµ‹åˆ°: {fully_detected_count}å¼  ({fully_detected_count/total_images*100:.1f}%)")
        if partially_detected > 0:
            print(f"    æœ‰é—æ¼: {partially_detected}å¼  ({partially_detected/total_images*100:.1f}%)")
        
        # éšæœºæŠ½å–10å¼ å›¾ç‰‡è¿›è¡Œå¯è§†åŒ–æµ‹è¯•
        print(f"\nğŸ“¸ éšæœºæŠ½å–10å¼ å›¾ç‰‡ç”Ÿæˆå¯è§†åŒ–æˆªå›¾...")
        sample_count = min(10, len(all_images))
        sample_images = random.sample(all_images, sample_count)
        
        # åˆ›å»ºæµ‹è¯•ç»“æœç›®å½•
        test_results_dir = Path(f"test_results/{self.page_type}_test_samples")
        
        # åˆ é™¤æ—§çš„æˆªå›¾
        if test_results_dir.exists():
            print(f"  ğŸ—‘ï¸  åˆ é™¤æ—§çš„æµ‹è¯•æˆªå›¾...")
            shutil.rmtree(test_results_dir)
        
        # åˆ›å»ºæ–°ç›®å½•
        test_results_dir.mkdir(parents=True, exist_ok=True)
        
        # å¯¹æŠ½å–çš„10å¼ å›¾ç‰‡ç”Ÿæˆå¯è§†åŒ–æˆªå›¾
        print(f"  ğŸ“· ç”Ÿæˆå¯è§†åŒ–æˆªå›¾...")
        sample_detection_stats = []
        for i, img_path in enumerate(sample_images, 1):
            # é¢„æµ‹
            pred_results = model.predict(str(img_path), conf=0.25, save=False, verbose=False)
            
            # ç»Ÿè®¡æ£€æµ‹ç»“æœ
            detections = pred_results[0].boxes
            detected_classes = {}
            for cls_id in detections.cls:
                cls_name = classes[int(cls_id)]
                detected_classes[cls_name] = detected_classes.get(cls_name, 0) + 1
            
            # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ç±»åˆ«éƒ½è¢«æ£€æµ‹åˆ°
            missing_classes = [cls for cls in classes if cls not in detected_classes]
            
            sample_detection_stats.append({
                'image': img_path.name,
                'detected': detected_classes,
                'missing': missing_classes,
                'total_detections': len(detections)
            })
            
            # ç»˜åˆ¶ç»“æœ
            annotated = pred_results[0].plot()
            
            # ä¿å­˜
            output_file = test_results_dir / f"test_{i:02d}_{img_path.name}"
            cv2.imwrite(str(output_file), annotated)
            
            # æ‰“å°æ£€æµ‹ç»“æœ
            status = "âœ“" if not missing_classes else "âš "
            print(f"    {status} å·²ä¿å­˜æµ‹è¯•æˆªå›¾ {i}/{sample_count}: {output_file.name}")
            print(f"       æ£€æµ‹åˆ°: {dict(detected_classes)}")
            if missing_classes:
                print(f"       é—æ¼: {missing_classes}")
        
        # ç»Ÿè®¡æŠ½æ ·æˆªå›¾çš„æ£€æµ‹æƒ…å†µ
        sample_fully_detected = sum(1 for stat in sample_detection_stats if not stat['missing'])
        sample_partially_detected = sample_count - sample_fully_detected
        
        print(f"\n  ğŸ“Š æŠ½æ ·æˆªå›¾æ£€æµ‹ç»Ÿè®¡:")
        print(f"    æµ‹è¯•å›¾ç‰‡: {sample_count}å¼ ")
        print(f"    å…¨éƒ¨æ£€æµ‹åˆ°: {sample_fully_detected}å¼  ({sample_fully_detected/sample_count*100:.1f}%)")
        if sample_partially_detected > 0:
            print(f"    æœ‰é—æ¼: {sample_partially_detected}å¼  ({sample_partially_detected/sample_count*100:.1f}%)")
        
        print(f"\n  âœ… æµ‹è¯•æˆªå›¾å·²ä¿å­˜åˆ°: {test_results_dir}")
        
        # æ‰“å¼€æˆªå›¾æ–‡ä»¶å¤¹
        print(f"\n  ğŸ“‚ æ‰“å¼€æˆªå›¾æ–‡ä»¶å¤¹...")
        try:
            if platform.system() == 'Windows':
                os.startfile(str(test_results_dir.absolute()))
            elif platform.system() == 'Darwin':  # macOS
                subprocess.run(['open', str(test_results_dir.absolute())])
            else:  # Linux
                subprocess.run(['xdg-open', str(test_results_dir.absolute())])
            print(f"  âœ“ å·²æ‰“å¼€æ–‡ä»¶å¤¹")
        except Exception as e:
            print(f"  âš  æ— æ³•è‡ªåŠ¨æ‰“å¼€æ–‡ä»¶å¤¹: {e}")
            print(f"  è¯·æ‰‹åŠ¨æ‰“å¼€: {test_results_dir.absolute()}")
        
        print(f"\nâœ… æµ‹è¯•å®Œæˆ!")
        
        # è¿”å›æµ‹è¯•ç»“æœå­—å…¸
        return {
            'mAP50': float(results.box.map50),
            'mAP50-95': float(results.box.map),
            'precision': float(results.box.mp),
            'recall': float(results.box.mr),
            'train_count': len(train_images),
            'val_count': len(val_images),
            'total_count': len(all_images),
            'test_samples_dir': str(test_results_dir),
            'all_images_stats': {
                'total_images': total_images,
                'fully_detected': fully_detected_count,
                'partially_detected': partially_detected,
                'detection_rate': fully_detected_count / total_images * 100
            },
            'sample_images_stats': {
                'total_images': sample_count,
                'fully_detected': sample_fully_detected,
                'partially_detected': sample_partially_detected,
                'detection_rate': sample_fully_detected / sample_count * 100
            }
        }
    
    def cleanup_after_training(self):
        """è®­ç»ƒåæ•´ç†ï¼šç§»åŠ¨åŸå§‹æ ‡æ³¨å›¾ã€åˆ é™¤å¢å¼ºæ•°æ®ã€æ³¨å†Œæ¨¡å‹"""
        print(f"\n{'='*60}")
        print(f"è®­ç»ƒåæ•´ç† - {self.page_type}")
        print(f"{'='*60}\n")
        
        cleanup_results = {
            'original_saved': False,
            'dataset_deleted': False,
            'model_registered': False,
            'report_generated': False
        }
        
        # 1. ç§»åŠ¨åŸå§‹æ ‡æ³¨å›¾åˆ° åŸå§‹æ ‡æ³¨å›¾/ ç›®å½•
        print(f"ğŸ“¦ [1/4] ä¿å­˜åŸå§‹æ ‡æ³¨å›¾...")
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        original_dir = Path(f"åŸå§‹æ ‡æ³¨å›¾/{self.page_type}_{timestamp}")
        original_dir.mkdir(parents=True, exist_ok=True)
        
        # å¤åˆ¶åŸå§‹å›¾ç‰‡å’Œæ ‡ç­¾
        annotation_file = self.source_dir / "annotations.json"
        copied_count = 0
        
        if annotation_file.exists():
            with open(annotation_file, 'r', encoding='utf-8') as f:
                annotations = json.load(f)
            
            # åˆ›å»ºå­ç›®å½•
            (original_dir / "images").mkdir(exist_ok=True)
            (original_dir / "labels").mkdir(exist_ok=True)
            
            # è·å–ç±»åˆ«æ˜ å°„
            classes = sorted(set(box['class'] for boxes in annotations.values() if boxes for box in boxes))
            class_to_id = {cls: idx for idx, cls in enumerate(classes)}
            
            # å¤åˆ¶åŸå§‹å›¾ç‰‡å’Œç”ŸæˆYOLOæ ‡ç­¾
            for img_path_str, boxes in annotations.items():
                if not boxes:
                    continue
                
                img_path = Path(img_path_str)
                if not img_path.exists():
                    continue
                
                # å¤åˆ¶å›¾ç‰‡
                shutil.copy2(img_path, original_dir / "images" / img_path.name)
                
                # ç”ŸæˆYOLOæ ‡ç­¾
                img = Image.open(img_path)
                img_width, img_height = img.size
                
                label_path = original_dir / "labels" / img_path.with_suffix(".txt").name
                with open(label_path, 'w') as f:
                    for box in boxes:
                        class_id = class_to_id[box['class']]
                        
                        # å¤„ç†ä¸¤ç§æ ‡æ³¨æ ¼å¼
                        if 'x' in box and 'width' in box:
                            # æ ¼å¼1: x, y, width, height
                            x_center = (box['x'] + box['width'] / 2) / img_width
                            y_center = (box['y'] + box['height'] / 2) / img_height
                            width = box['width'] / img_width
                            height = box['height'] / img_height
                        elif 'x1' in box and 'x2' in box:
                            # æ ¼å¼2: x1, y1, x2, y2
                            x_center = ((box['x1'] + box['x2']) / 2) / img_width
                            y_center = ((box['y1'] + box['y2']) / 2) / img_height
                            width = (box['x2'] - box['x1']) / img_width
                            height = (box['y2'] - box['y1']) / img_height
                        else:
                            continue
                        
                        f.write(f"{class_id} {x_center} {y_center} {width} {height}\n")
                
                copied_count += 1
            
            # å¤åˆ¶annotations.json
            shutil.copy2(annotation_file, original_dir / "annotations.json")
            
            print(f"  âœ“ å·²ä¿å­˜ {copied_count} å¼ åŸå§‹æ ‡æ³¨å›¾åˆ°: {original_dir}")
            cleanup_results['original_saved'] = True
        else:
            print(f"  âš  æœªæ‰¾åˆ°æ ‡æ³¨æ–‡ä»¶ï¼Œè·³è¿‡")
        
        # 2. åˆ é™¤YOLOæ•°æ®é›†ï¼ˆå¢å¼ºæ•°æ®ï¼‰
        print(f"\nğŸ—‘ï¸  [2/4] åˆ é™¤YOLOæ•°æ®é›†ï¼ˆå¢å¼ºæ•°æ®ï¼‰...")
        if self.dataset_dir.exists():
            try:
                shutil.rmtree(self.dataset_dir)
                print(f"  âœ“ å·²åˆ é™¤: {self.dataset_dir}")
                cleanup_results['dataset_deleted'] = True
            except Exception as e:
                print(f"  âœ— åˆ é™¤å¤±è´¥: {e}")
        else:
            print(f"  âš  æ•°æ®é›†ä¸å­˜åœ¨ï¼Œè·³è¿‡")
            cleanup_results['dataset_deleted'] = True  # ä¸å­˜åœ¨ä¹Ÿç®—æˆåŠŸ
        
        # 3. æ³¨å†Œæ¨¡å‹åˆ° yolo_model_registry.json
        print(f"\nğŸ“ [3/4] æ³¨å†Œæ¨¡å‹...")
        registry_file = Path("yolo_model_registry.json")
        
        # è¯»å–ç°æœ‰æ³¨å†Œè¡¨
        if registry_file.exists():
            with open(registry_file, 'r', encoding='utf-8') as f:
                registry = json.load(f)
        else:
            registry = {
                "models": {},
                "usage": {
                    "description": "YOLOæ¨¡å‹æ³¨å†Œè¡¨ï¼Œè®°å½•æ‰€æœ‰è®­ç»ƒå®Œæˆçš„æ¨¡å‹ä¿¡æ¯",
                    "how_to_use": {
                        "load_model": "from ultralytics import YOLO; model = YOLO(registry['models']['homepage']['model_path'])",
                        "get_classes": "classes = registry['models']['homepage']['classes']",
                        "check_performance": "performance = registry['models']['homepage']['performance']"
                    }
                },
                "version": "1.0"
            }
        
        # æŸ¥æ‰¾æœ€æ–°çš„æ¨¡å‹
        model_pattern = f"runs/detect/**/yolo_runs/{self.model_name}*/weights/best.pt"
        model_files = glob.glob(model_pattern, recursive=True)
        
        if model_files:
            model_path = sorted(model_files, key=lambda x: Path(x).stat().st_mtime)[-1]
            
            # ä»åŸå§‹æ ‡æ³¨ä¸­è·å–ç±»åˆ«
            if annotation_file.exists():
                with open(annotation_file, 'r', encoding='utf-8') as f:
                    annotations = json.load(f)
                classes = sorted(set(box['class'] for boxes in annotations.values() if boxes for box in boxes))
                original_count = sum(1 for v in annotations.values() if v)
            else:
                classes = []
                original_count = 0
            
            # æµ‹è¯•æ¨¡å‹è·å–æ€§èƒ½æŒ‡æ ‡
            model = YOLO(model_path)
            
            # å°è¯•ä»å·²å­˜åœ¨çš„æ•°æ®é›†æµ‹è¯•ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è·³è¿‡æ€§èƒ½æµ‹è¯•
            yaml_path_for_test = self.dataset_dir / "dataset.yaml"
            
            if yaml_path_for_test.exists():
                try:
                    results = model.val(data=str(yaml_path_for_test))
                    performance = {
                        "mAP50": float(results.box.map50),
                        "mAP50-95": float(results.box.map),
                        "precision": float(results.box.mp),
                        "recall": float(results.box.mr)
                    }
                except Exception as e:
                    print(f"  âš  æ— æ³•æµ‹è¯•æ¨¡å‹æ€§èƒ½: {e}")
                    performance = {
                        "mAP50": 0.0,
                        "mAP50-95": 0.0,
                        "precision": 0.0,
                        "recall": 0.0
                    }
            else:
                print(f"  âš  æ•°æ®é›†ä¸å­˜åœ¨ï¼Œè·³è¿‡æ€§èƒ½æµ‹è¯•")
                performance = {
                    "mAP50": 0.0,
                    "mAP50-95": 0.0,
                    "precision": 0.0,
                    "recall": 0.0
                }
            
            # æ·»åŠ åˆ°æ³¨å†Œè¡¨
            model_key = self.page_type.lower().replace(" ", "_")
            registry["models"][model_key] = {
                "name": f"{self.page_type}æ£€æµ‹æ¨¡å‹",
                "page_type": self.page_type,
                "model_path": model_path,
                "classes": classes,
                "num_classes": len(classes),
                "performance": performance,
                "training_date": datetime.now().strftime("%Y-%m-%d"),
                "dataset_size": {
                    "original": original_count,
                    "augmented": 0,  # å·²åˆ é™¤
                    "train": 0,
                    "val": 0
                },
                "original_data_path": str(original_dir),
                "notes": f"ä½¿ç”¨yolo_train_manager.pyè®­ç»ƒï¼Œé»˜è®¤50è½®"
            }
            
            # æ›´æ–°last_updated
            registry["last_updated"] = datetime.now().strftime("%Y-%m-%d")
            
            # ä¿å­˜æ³¨å†Œè¡¨
            with open(registry_file, 'w', encoding='utf-8') as f:
                json.dump(registry, f, ensure_ascii=False, indent=2)
            
            print(f"  âœ“ å·²æ³¨å†Œæ¨¡å‹: {model_key}")
            print(f"  æ¨¡å‹è·¯å¾„: {model_path}")
            print(f"  ç±»åˆ«: {classes}")
            print(f"  æ€§èƒ½: mAP50={performance['mAP50']:.3f}, Precision={performance['precision']:.3f}, Recall={performance['recall']:.3f}")
            cleanup_results['model_registered'] = True
        else:
            print(f"  âœ— æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œè·³è¿‡æ³¨å†Œ")
        
        # 4. ç”Ÿæˆæ•´ç†æŠ¥å‘Š
        print(f"\nğŸ“„ [4/4] ç”Ÿæˆæ•´ç†æŠ¥å‘Š...")
        report_path = Path(f"training_reports/{self.page_type}_{timestamp}.txt")
        report_path.parent.mkdir(exist_ok=True)
        
        try:
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write("=" * 60 + "\n")
                f.write(f"{self.page_type} è®­ç»ƒå®ŒæˆæŠ¥å‘Š\n")
                f.write("=" * 60 + "\n\n")
                f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
                
                f.write("1. åŸå§‹æ ‡æ³¨å›¾\n")
                f.write(f"   ä½ç½®: {original_dir}\n")
                f.write(f"   æ•°é‡: {copied_count if 'copied_count' in locals() else 0} å¼ \n\n")
                
                f.write("2. æ¨¡å‹ä¿¡æ¯\n")
                if model_files:
                    f.write(f"   è·¯å¾„: {model_path}\n")
                    f.write(f"   ç±»åˆ«: {', '.join(classes)}\n")
                    f.write(f"   æ€§èƒ½:\n")
                    f.write(f"     mAP50: {performance['mAP50']:.3f}\n")
                    f.write(f"     mAP50-95: {performance['mAP50-95']:.3f}\n")
                    f.write(f"     Precision: {performance['precision']:.3f}\n")
                    f.write(f"     Recall: {performance['recall']:.3f}\n\n")
                
                f.write("3. æ•°æ®æ¸…ç†\n")
                f.write(f"   âœ“ å·²åˆ é™¤YOLOæ•°æ®é›†: {self.dataset_dir}\n")
                f.write(f"   âœ“ å·²ä¿å­˜åŸå§‹æ ‡æ³¨å›¾: {original_dir}\n")
                f.write(f"   âœ“ å·²æ³¨å†Œæ¨¡å‹åˆ°: yolo_model_registry.json\n\n")
                
                f.write("=" * 60 + "\n")
            
            print(f"  âœ“ æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
            cleanup_results['report_generated'] = True
        except Exception as e:
            print(f"  âœ— ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {e}")
        
        # éªŒè¯æ•´ç†æ˜¯å¦å®Œæˆ
        print(f"\n{'='*60}")
        print(f"ğŸ” éªŒè¯æ•´ç†ç»“æœ")
        print(f"{'='*60}\n")
        
        all_success = True
        
        # éªŒè¯1: åŸå§‹æ ‡æ³¨å›¾æ˜¯å¦ä¿å­˜
        if cleanup_results['original_saved']:
            images_count = len(list((original_dir / "images").glob("*")))
            labels_count = len(list((original_dir / "labels").glob("*.txt")))
            if images_count > 0 and labels_count > 0:
                print(f"  âœ“ åŸå§‹æ ‡æ³¨å›¾å·²ä¿å­˜: {images_count}å¼ å›¾ç‰‡, {labels_count}ä¸ªæ ‡ç­¾")
            else:
                print(f"  âœ— åŸå§‹æ ‡æ³¨å›¾ä¿å­˜ä¸å®Œæ•´: {images_count}å¼ å›¾ç‰‡, {labels_count}ä¸ªæ ‡ç­¾")
                all_success = False
        else:
            print(f"  âœ— åŸå§‹æ ‡æ³¨å›¾æœªä¿å­˜")
            all_success = False
        
        # éªŒè¯2: YOLOæ•°æ®é›†æ˜¯å¦åˆ é™¤
        if cleanup_results['dataset_deleted']:
            if not self.dataset_dir.exists():
                print(f"  âœ“ YOLOæ•°æ®é›†å·²åˆ é™¤")
            else:
                print(f"  âœ— YOLOæ•°æ®é›†ä»ç„¶å­˜åœ¨: {self.dataset_dir}")
                all_success = False
        else:
            print(f"  âœ— YOLOæ•°æ®é›†æœªåˆ é™¤")
            all_success = False
        
        # éªŒè¯3: æ¨¡å‹æ˜¯å¦æ³¨å†Œ
        if cleanup_results['model_registered']:
            if registry_file.exists():
                with open(registry_file, 'r', encoding='utf-8') as f:
                    registry = json.load(f)
                model_key = self.page_type.lower().replace(" ", "_")
                if model_key in registry.get("models", {}):
                    print(f"  âœ“ æ¨¡å‹å·²æ³¨å†Œ: {model_key}")
                else:
                    print(f"  âœ— æ¨¡å‹æœªåœ¨æ³¨å†Œè¡¨ä¸­æ‰¾åˆ°: {model_key}")
                    all_success = False
            else:
                print(f"  âœ— æ¨¡å‹æ³¨å†Œè¡¨ä¸å­˜åœ¨")
                all_success = False
        else:
            print(f"  âœ— æ¨¡å‹æœªæ³¨å†Œ")
            all_success = False
        
        # éªŒè¯4: æŠ¥å‘Šæ˜¯å¦ç”Ÿæˆ
        if cleanup_results['report_generated']:
            if report_path.exists():
                print(f"  âœ“ æ•´ç†æŠ¥å‘Šå·²ç”Ÿæˆ")
            else:
                print(f"  âœ— æ•´ç†æŠ¥å‘Šä¸å­˜åœ¨")
                all_success = False
        else:
            print(f"  âœ— æ•´ç†æŠ¥å‘Šæœªç”Ÿæˆ")
            all_success = False
        
        print(f"\n{'='*60}")
        if all_success:
            print(f"âœ… æ•´ç†å®Œæˆï¼æ‰€æœ‰æ“ä½œå‡æˆåŠŸ")
        else:
            print(f"âš ï¸  æ•´ç†å®Œæˆï¼Œä½†éƒ¨åˆ†æ“ä½œå¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¸Šè¿°é”™è¯¯")
        print(f"{'='*60}")
        print(f"åŸå§‹æ ‡æ³¨å›¾: {original_dir}")
        print(f"æ¨¡å‹æ³¨å†Œè¡¨: yolo_model_registry.json")
        print(f"æ•´ç†æŠ¥å‘Š: {report_path}")
        
        return all_success


def main():
    parser = argparse.ArgumentParser(description='YOLOè®­ç»ƒç®¡ç†å™¨')
    parser.add_argument('action', choices=['prepare', 'train', 'test', 'cleanup', 'all'], 
                       help='æ“ä½œ: prepare(å‡†å¤‡æ•°æ®), train(è®­ç»ƒ), test(æµ‹è¯•), cleanup(æ•´ç†), all(å…¨éƒ¨)')
    parser.add_argument('page_type', help='é¡µé¢ç±»å‹ï¼Œå¦‚"åˆ†ç±»é¡µ"ã€"ç™»å½•é¡µ"')
    parser.add_argument('--epochs', type=int, default=50, help='è®­ç»ƒè½®æ•°(é»˜è®¤50)')
    parser.add_argument('--augment', type=int, default=None, help='æ•°æ®å¢å¼ºå€æ•°(é»˜è®¤è‡ªåŠ¨)')
    
    args = parser.parse_args()
    
    manager = YOLOTrainManager(args.page_type)
    
    if args.action == 'prepare':
        manager.prepare_dataset(args.augment)
    elif args.action == 'train':
        manager.train_model(args.epochs)
    elif args.action == 'test':
        manager.test_model()
    elif args.action == 'cleanup':
        manager.cleanup_after_training()
    elif args.action == 'all':
        if manager.prepare_dataset(args.augment):
            if manager.train_model(args.epochs):
                test_results = manager.test_model()
                if test_results:
                    # æ±‡æŠ¥æµ‹è¯•ç»“æœï¼Œç­‰å¾…ç”¨æˆ·æ‰¹å‡†
                    print(f"\n{'='*60}")
                    print(f"âš ï¸  ç­‰å¾…ç”¨æˆ·æ‰¹å‡†")
                    print(f"{'='*60}")
                    print(f"\næµ‹è¯•å·²å®Œæˆï¼Œæ€§èƒ½æŒ‡æ ‡å¦‚ä¸‹ï¼š")
                    print(f"  mAP50: {test_results['mAP50']:.3f}")
                    print(f"  mAP50-95: {test_results['mAP50-95']:.3f}")
                    print(f"  Precision: {test_results['precision']:.3f}")
                    print(f"  Recall: {test_results['recall']:.3f}")
                    print(f"\næµ‹è¯•äº†æ‰€æœ‰å›¾ç‰‡ï¼ˆåŒ…æ‹¬å¢å¼ºå›¾ï¼‰ï¼š")
                    print(f"  è®­ç»ƒé›†: {test_results['train_count']}å¼ ")
                    print(f"  éªŒè¯é›†: {test_results['val_count']}å¼ ")
                    print(f"  æ€»è®¡: {test_results['total_count']}å¼ ")
                    print(f"\næ‰€æœ‰å›¾ç‰‡æ£€æµ‹ç»“æœï¼š")
                    print(f"  æµ‹è¯•å›¾ç‰‡: {test_results['all_images_stats']['total_images']}å¼ ")
                    print(f"  å…¨éƒ¨æ£€æµ‹åˆ°: {test_results['all_images_stats']['fully_detected']}å¼  ({test_results['all_images_stats']['detection_rate']:.1f}%)")
                    if test_results['all_images_stats']['partially_detected'] > 0:
                        print(f"  æœ‰é—æ¼: {test_results['all_images_stats']['partially_detected']}å¼  ({100-test_results['all_images_stats']['detection_rate']:.1f}%)")
                    print(f"\néšæœºæŠ½å–çš„10å¼ æµ‹è¯•æˆªå›¾æ£€æµ‹ç»“æœï¼š")
                    print(f"  æµ‹è¯•å›¾ç‰‡: {test_results['sample_images_stats']['total_images']}å¼ ")
                    print(f"  å…¨éƒ¨æ£€æµ‹åˆ°: {test_results['sample_images_stats']['fully_detected']}å¼  ({test_results['sample_images_stats']['detection_rate']:.1f}%)")
                    if test_results['sample_images_stats']['partially_detected'] > 0:
                        print(f"  æœ‰é—æ¼: {test_results['sample_images_stats']['partially_detected']}å¼ ")
                    print(f"  ä½ç½®: {test_results['test_samples_dir']}")
                    print(f"  è¯·æŸ¥çœ‹æˆªå›¾ä»¥åˆ¤æ–­æ¨¡å‹æ˜¯å¦æ­£ç¡®åŒ¹é…")
                    print(f"\nå¦‚éœ€æ‰§è¡Œæœ€ç»ˆæ•´ç†ï¼ˆåˆ é™¤YOLOæ•°æ®é›†ï¼‰ï¼Œè¯·è¿è¡Œï¼š")
                    print(f"  python yolo_train_manager.py cleanup {args.page_type}")
                    print(f"\n{'='*60}")


if __name__ == "__main__":
    main()
