"""
å¿«é€Ÿæ£€æŸ¥GPUçŠ¶æ€
"""
import torch
import time

print("=" * 60)
print("ğŸ” GPUçŠ¶æ€æ£€æŸ¥")
print("=" * 60)

# æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨
print(f"\n1. CUDAå¯ç”¨: {torch.cuda.is_available()}")

if torch.cuda.is_available():
    print(f"2. GPUè®¾å¤‡æ•°é‡: {torch.cuda.device_count()}")
    print(f"3. å½“å‰GPU: {torch.cuda.current_device()}")
    print(f"4. GPUåç§°: {torch.cuda.get_device_name(0)}")
    print(f"5. CUDAç‰ˆæœ¬: {torch.version.cuda}")
    print(f"6. cuDNNç‰ˆæœ¬: {torch.backends.cudnn.version()}")
    print(f"7. cuDNNå¯ç”¨: {torch.backends.cudnn.enabled}")
    
    # æ˜¾å­˜ä¿¡æ¯
    total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
    allocated = torch.cuda.memory_allocated(0) / 1024**3
    cached = torch.cuda.memory_reserved(0) / 1024**3
    
    print(f"\næ˜¾å­˜ä¿¡æ¯:")
    print(f"  â€¢ æ€»æ˜¾å­˜: {total_memory:.2f} GB")
    print(f"  â€¢ å·²åˆ†é…: {allocated:.2f} GB")
    print(f"  â€¢ å·²ç¼“å­˜: {cached:.2f} GB")
    print(f"  â€¢ å¯ç”¨: {total_memory - allocated:.2f} GB")
    
    # æ€§èƒ½æµ‹è¯•
    print(f"\næ€§èƒ½æµ‹è¯•:")
    
    # CPUæµ‹è¯•
    print(f"  æµ‹è¯•1: CPUçŸ©é˜µä¹˜æ³•...")
    cpu_tensor = torch.randn(2000, 2000)
    start = time.time()
    for _ in range(10):
        _ = cpu_tensor @ cpu_tensor
    cpu_time = time.time() - start
    print(f"    CPUè€—æ—¶: {cpu_time:.3f}ç§’")
    
    # GPUæµ‹è¯•
    print(f"  æµ‹è¯•2: GPUçŸ©é˜µä¹˜æ³•...")
    gpu_tensor = torch.randn(2000, 2000).cuda()
    torch.cuda.synchronize()
    start = time.time()
    for _ in range(10):
        _ = gpu_tensor @ gpu_tensor
    torch.cuda.synchronize()
    gpu_time = time.time() - start
    print(f"    GPUè€—æ—¶: {gpu_time:.3f}ç§’")
    
    speedup = cpu_time / gpu_time
    print(f"\n  âš¡ GPUåŠ é€Ÿæ¯”: {speedup:.1f}x")
    
    if speedup < 2:
        print(f"\n  âš ï¸  è­¦å‘Š: GPUåŠ é€Ÿæ•ˆæœä¸æ˜æ˜¾")
        print(f"     å¯èƒ½åŸå› :")
        print(f"     1. ä½¿ç”¨çš„æ˜¯é›†æˆæ˜¾å¡æˆ–ä½ç«¯æ˜¾å¡")
        print(f"     2. GPUé©±åŠ¨æœªæ­£ç¡®å®‰è£…")
        print(f"     3. PyTorchæœªæ­£ç¡®å®‰è£…CUDAç‰ˆæœ¬")
    else:
        print(f"\n  âœ“ GPUå·¥ä½œæ­£å¸¸!")
    
    # æ··åˆç²¾åº¦æµ‹è¯•
    print(f"\n  æµ‹è¯•3: æ··åˆç²¾åº¦è®­ç»ƒ(AMP)...")
    try:
        scaler = torch.cuda.amp.GradScaler()
        with torch.cuda.amp.autocast():
            result = gpu_tensor @ gpu_tensor
        print(f"    âœ“ AMPæ”¯æŒæ­£å¸¸")
    except Exception as e:
        print(f"    âœ— AMPä¸æ”¯æŒ: {e}")
    
else:
    print("\nâŒ æœªæ£€æµ‹åˆ°CUDAæ”¯æŒçš„GPU")
    print("\nå¯èƒ½çš„åŸå› :")
    print("1. æœªå®‰è£…CUDAç‰ˆæœ¬çš„PyTorch")
    print("2. æ˜¾å¡é©±åŠ¨æœªæ­£ç¡®å®‰è£…")
    print("3. æ˜¾å¡ä¸æ”¯æŒCUDA")
    
    print("\nè§£å†³æ–¹æ¡ˆ:")
    print("1. å¸è½½å½“å‰PyTorch:")
    print("   pip uninstall torch torchvision")
    print("\n2. å®‰è£…CUDAç‰ˆæœ¬çš„PyTorch:")
    print("   pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118")
    print("\n3. æ£€æŸ¥æ˜¾å¡é©±åŠ¨:")
    print("   è¿è¡Œ nvidia-smi æŸ¥çœ‹æ˜¾å¡çŠ¶æ€")

print("\n" + "=" * 60)
